% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/benchmark.R
\name{Benchmark}
\alias{Benchmark}
\title{Define a Benchmark}
\usage{
Benchmark(
  name,
  setup = function(...) BenchEnvironment(...),
  before_each = TRUE,
  run = TRUE,
  after_each = TRUE,
  teardown = TRUE,
  valid_params = function(params) params,
  case_version = function(params) NULL,
  batch_id_fun = function(params) uuid(),
  tags_fun = function(params) params,
  packages_used = function(params) "arrow",
  ...
)
}
\arguments{
\item{name}{string identifier for the benchmark, included in results}

\item{setup}{function having as its arguments the benchmark parameters. See
the \verb{Parametrizing benchmarks} section. This function is called once
to initialize the benchmark context for a given set of parameters.
It should return \code{\link[=BenchEnvironment]{BenchEnvironment()}} with any parameter values or resources
that the other expressions will need to run.}

\item{before_each}{expression that is evaluated before every iteration. You may
not need to define one.}

\item{run}{expression that executes what we want to measure (and nothing more).
Only code in this function is benchmarked.}

\item{after_each}{expression evaluated after every iteration. You can put here
assertions about the result of \code{run()}--errors in this function will fail
the benchmark (not record results)}

\item{teardown}{expression evaluated after all iterations are complete. Use this to
clean up any artifacts created, for example. This function may error without
affecting the benchmark results}

\item{valid_params}{function taking a \code{data.frame} of setup parameters and
returning a \code{data.frame} of setup parameters. Use this to filter out invalid
combinations of parameters (e.g. writing a Feather file with snappy
compression, which is unsupported)}

\item{case_version}{function taking a named list of setup parameters for a
single case and returning an integer version for the case, or \code{NULL} to not
append a version; \code{NA} will raise an error. Changes to version will break
conbench history for a case.}

\item{batch_id_fun}{A unary function which takes a dataframe of parameters and
returns a character vector to use as \code{batch_id} of length 1 or \code{nrow(params)}}

\item{tags_fun}{A unary function which takes a named list of setup parameters
for a single case and returns a named list of tags to send to Conbench for that
case. Can be overwritten to add static tags or postprocess parameters into more
readable forms.}

\item{packages_used}{function taking a \code{data.frame} of setup parameters and
returning a vector of R package names required}

\item{...}{additional attributes or functions, possibly called in \code{setup()}.}
}
\value{
A \code{Benchmark} object containing these functions
}
\description{
Define a Benchmark
}
\section{Evaluation}{

A \code{Benchmark} is evaluated something like:

\if{html}{\out{<div class="sourceCode">}}\preformatted{env <- bm$setup(param1 = "value", param2 = "value")
for (i in seq_len(n_iter)) \{
  eval(bm$before_each, envir = env)
  measure(eval(bm$run, envir = env))
  eval(bm$after_each, envir = env)
\}
eval(bm$teardown, envir = env)
}\if{html}{\out{</div>}}

Benchmarks should run a single combination of parameters. Running across
a range of parameter combinations is handled by the runner, not the functions
in the benchmark object.
}

\section{Parameterizing benchmarks}{


When we benchmark, we often want to run our code compared with someone else's
code, or we want to run our code but with different settings. There are a few
types of parameters that get expressed differently.

Function parameters, including both arguments to functions we're benchmarking
and the choice of function themselves, are expressed as arguments to the
\code{setup()} function. They should be simple values (strings or numbers) that
can easily be expressed in a configuration object or file. Things like R
functions that are parameters should be mapped to string identifiers and
dereferenced inside the \code{setup()} function.

Where appropriate, you should enumerate all possible parameter values and
use \code{match.arg()} in your \code{setup()} function to select and validate. By
listing the possible values in the function signature, \code{run_benchmark} can
identify the full parameter space to test by the Cartesian product of the
defaults; this set of default parameters can be filtered down by defining a
\code{valid_params()} function for your benchmark. If you do not provide default
parameter values, you will be required to specify them at runtime.

Some global or session parameters are managed outside of the Benchmark object
and do not require handling inside the benchmark's functions.
These are options that would apply to all benchmarks. Currently supported
R session parameters in \code{run_benchmark()}:
\itemize{
\item \code{lib_path}: To test different library versions, install into different
lib directories and provide the directories as the \code{lib_path} parameters.
They will be passed to \code{.libPaths()} at the beginning of each run. The
default \code{lib_path} is "latest", which doesn't set a special library path.
That is, by omitting \code{lib_path}, you're assuming that packages have been
installed outside of this process.
\item \code{cpu_count}: To restrict the number of threads available for computation,
specify an integer \code{cpu_count}. This sets the R \code{Ncpus} option, which many
packages follow, and also caps the \code{arrow} threadpool size.
\item \code{mem_alloc}: The memory allocator to be tested (one of: "jemalloc",
"mimalloc", "system")
\item \code{drop_caches}: Attempt to drop the disk cache before each case or iteration.
Currently only works on linux. Permissible values are \code{"case"}, \code{"iteration"},
and \code{NULL}. Defaults to \code{NULL}, i.e. don't drop caches.
}

Because these parameters can alter the global session state in unpredictable
ways, when we run benchmarks, we always do so by calling out to a fresh R
subprocess. That way, there is no potential contamination.

Any other R \code{options()} or environment variables that affect behavior under
test, specific to this Benchmark, can be specified as \code{setup()} parameters
and set inside that function. Be sure to restore any previous settings in
the \code{teardown()} function.
}

