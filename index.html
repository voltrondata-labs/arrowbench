<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Tools for Continuous and Interactive Benchmarking • arrowbench</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="bootstrap-toc.css">
<script src="bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="pkgdown.css" rel="stylesheet">
<script src="pkgdown.js"></script><meta property="og:title" content="Tools for Continuous and Interactive Benchmarking">
<meta property="og:description" content="Tools for defining benchmarks, running them across a 
    range of parameters, and reporting their results in a standardized form.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-home">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="index.html">arrowbench</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="contents col-md-9">

<div class="section level1">
<div class="page-header"><h1 id="arrowbench">arrowbench<a class="anchor" aria-label="anchor" href="#arrowbench"></a>
</h1></div>
<p><!-- badges: start --> <a href="https://github.com/ursa-labs/arrowbench/actions" class="external-link"><img src="https://github.com/ursacomputing/arrowbench/workflows/R-CMD-check/badge.svg" alt="R-CMD-check"></a> <!-- badges: end --></p>
<p>This R package contains tools for defining benchmarks, running them across a range of parameters, and reporting their results in a standardized form. It also contains some benchmark code for measuring performance of Apache Arrow and other projects one might compare it to.</p>
<p>The purpose of the package is to provide developers with better tools for creating, parametrizing, and reproducing benchmarks across a range of library versions, variables, and machines, as well as to facilitate continuous monitoring. While this package could be used for microbenchmarking, it is designed specially for “macrobenchmarks”: workflows that real users do with real data that take longer than microseconds to run.</p>
<p>It builds on top of existing R benchmarking tools, notably the <code>bench</code> package. Among the features that this package adds are</p>
<ul>
<li>Setup designed with parametrization in mind so you can test across a range of variables, which may not all be valid in combination</li>
<li>Isolation of benchmark runs in separate processes to prevent cross-contamination such as global environment changes and previous memory allocation</li>
<li>Tools for bootstrapping package versions and known data sources to facilitate running the same code on different machines</li>
</ul>
</div>
<div class="section level1">
<h1 id="user-guide">User guide<a class="anchor" aria-label="anchor" href="#user-guide"></a>
</h1>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>The quickest and easiest way to install is to run <code>remotes::install_github("ursacomputing/arrowbench", dependencies = TRUE)</code> in R. If you need to install remotes you can <code>install.packages("remotes")</code>.</p>
<p>If you’ve downloaded the source, or you’re making changes to arrow bench you should make sure that you have the dependencies with <code>remotes::install_deps(".", dependencies = TRUE)</code> in R (this will also install the arrow package along with other packages that can be benchmarked with arrowbench. And then running <code>R CMD INSTALL .</code> in a terminal (for both, you should do this in the root directory of arrowbench, or pass the path to arrowbench instead of <code>.</code>).</p>
<p>Some benchmark data files are downloaded with <code>download.file(..., method = "wget")</code>, which requires <a href="https://www.gnu.org/software/wget/" class="external-link">wget</a> to be installed. If not already installed, <code>wget</code> is available via most package managers, e.g. with <code>brew install wget</code> with Hombrew on MacOS.</p>
</div>
<div class="section level2">
<h2 id="contributing">Contributing<a class="anchor" aria-label="anchor" href="#contributing"></a>
</h2>
<p>To run DuckDB tests, set <code>ARROWBENCH_TEST_CUSTOM_DUCKDB</code> to <code>1</code> or another non-empty value in <code>~/.Renviron</code> or elsewhere such that it will be set during testing.</p>
</div>
<div class="section level2">
<h2 id="running-benchmarks">Running benchmarks<a class="anchor" aria-label="anchor" href="#running-benchmarks"></a>
</h2>
<p>Pass a Benchmark to <code><a href="reference/run_benchmark.html">run_benchmark()</a></code> and it will run it across the range of parameters specified. For parameters specified in <code>bm$setup</code> that are omitted when calling <code>run_benchmark(bm)</code>, it will test across all combinations of them (what we call a benchmark matrix). If some parameter combinations are not valid, define a <code>bm$valid_params(params)</code> function that will filter that expanded <code>data.frame</code> of parameters down to the valid set.</p>
<p>For example,</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">arrowbench</span><span class="op">)</span>

<span class="fu"><a href="reference/run_benchmark.html">run_benchmark</a></span><span class="op">(</span><span class="va">write_file</span>, source <span class="op">=</span> <span class="st">"nyctaxi_2010-01"</span><span class="op">)</span></code></pre></div>
<p>will run the <code>write_file</code> benchmark with “nyctaxi_2010-01” source file on the Cartesian product of the other function parameters–<code>format</code>, <code>compression</code>, and <code>input</code>–along with <code>cpu_count</code>s of <code>c(1, Ncpus)</code>.</p>
<p>Another example:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">arrowbench</span><span class="op">)</span>

<span class="fu"><a href="reference/run_benchmark.html">run_benchmark</a></span><span class="op">(</span><span class="va">write_file</span>, source <span class="op">=</span> <span class="st">"nyctaxi_2010-01"</span>, writer <span class="op">=</span> <span class="st">"feather"</span>, 
  input <span class="op">=</span> <span class="st">"data.frame"</span>, cpu_count <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">8</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>will run only the Feather writing tests with the two valid compression variants, each one done for 1, 4, and 8 threads, for a total of 6 runs.</p>
<p>If <code>lib_path</code> is not provided to <code><a href="reference/run_benchmark.html">run_benchmark()</a></code>, it will use the default <code>.libPath</code> and whatever is installed there. You can also indicate a subset of released <code>x.y</code> Arrow version numbers, or <code>lib_path = "all"</code> to test all past releases of <code>arrow</code> plus <code>"latest"</code>.</p>
<div class="section level3">
<h3 id="run-options">Run options<a class="anchor" aria-label="anchor" href="#run-options"></a>
</h3>
<p><code><a href="reference/run_benchmark.html">run_benchmark()</a></code> handles executing benchmarks across a range of parameters. After determining the valid parameters, it calls <code><a href="reference/run_one.html">run_one()</a></code> on each and collects the results. <code><a href="reference/run_one.html">run_one()</a></code> generates an R script and then shells out to a separate R process to execute the benchmark, then collects the results from it.</p>
<p>You may call <code><a href="reference/run_one.html">run_one()</a></code> directly. It takes some options, which may be passed from <code><a href="reference/run_benchmark.html">run_benchmark()</a></code> (both default <code>FALSE</code>):</p>
<ul>
<li>
<code>n_iter</code>: the number of iterations to run the specific benchmark (default: 3)</li>
<li>
<code>dry_run</code>: logical, returns the R script instead of executing it. Useful for debugging, though you probably don’t want to execute the script yourself in order to do the benchmarking: <code>run_script()</code>, which <code><a href="reference/run_one.html">run_one()</a></code> calls when <code>dry_run = FALSE</code>, has some useful wrapping for caching and collecting results.</li>
<li>
<code>profiling</code>: logical, allows you you instrument the R code and collect profiling data. You don’t want to do this if you’re collecting benchmark data because the profiler can add to the run time, but if you see something slow and want to explore why, this can be a good start. Note that this doesn’t do profiling at the C++ level, so if something is slow inside an arrow C++ function, this won’t tell you what exactly, but it can help rule things out. If <code>TRUE</code>, the result data will contain a <code>prof_file</code> field, which you can read in with <code>profvis::profvis(prof_input = file)</code>.</li>
<li>
<code>read_only</code>: don’t actually run benchmarks, but read any results that are in the results directory.</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="defining-benchmarks">Defining benchmarks<a class="anchor" aria-label="anchor" href="#defining-benchmarks"></a>
</h2>
<p>Benchmarks are constructed by <code><a href="reference/Benchmark.html">Benchmark()</a></code>, which takes expressions that handle setup, teardown, and the actual work that we want to measure. See its documentation for details, and see <code>read_file</code> and <code>write_file</code> for examples.</p>
<div class="section level3">
<h3 id="a-contrived-example">A (contrived) example<a class="anchor" aria-label="anchor" href="#a-contrived-example"></a>
</h3>
<p>Here we create a new kind of csv benchmark that uses <code><a href="https://arrow.apache.org/docs/r/reference/read_delim_arrow.html" class="external-link">arrow::read_csv_arrow()</a></code> and varies the arguments <code>as_data_frame</code> and <code>skip_empty_rows</code>. This is a bit of a contrived example, see <code>read_csv</code> for how we actually test csv reading. There are comments in the code block explaining what each section does.</p>
<pre><code>new_csv_benchmark &lt;- Benchmark(
  "new_csv_benchmark",
  # This setup block will be run before the benchmark is started. This is run
  # before each case / single item in the benchmark matrix, so is a good time
  # to setup case- or source-specific properties (see `result_dim` below).
  setup = function(source = names(known_sources),
                   as_data_frame = c(TRUE, FALSE),
                   skip_empty_rows = TRUE)) {
    # Validate the parameters
    # For our benchmark: as_data_frame defaults to TRUE and FALSE (so if it is 
    # unspecified you will get both TRUE and FALSE in the benchmark matrix)
    as_data_frame &lt;- match.arg(as_data_frame)

    # For our benchmark: skip_empty_rows defaults to TRUE (so if it is 
    # unspecified you will only get TRUE in the benchmark matrix), but it can 
    # accept TRUE or FALSE, so we validate that it is one of those.
    skip_empty_rows &lt;- match.arg(skip_empty_rows, c(TRUE, FALSE))

    # Ensure the file exists as an uncompressed csv
    input_file &lt;- ensure_format(source, "csv", "uncompressed")

    # Extract the dim attribute from a data source for validation later
    result_dim &lt;- get_source_attr(source, "dim")

    # Finally we return a `BenchEnvironment` with the parameters we defined 
    # above that are needed
    BenchEnvironment(
      input_file = input_file,
      result_dim = result_dim,
      as_data_frame = as_data_frame,
      skip_empty_rows = skip_empty_rows
    )
  },
  # This is run before each iteration. 
  before_each = {
    # Make sure the result is cleared
    result &lt;- NULL
  },
  # This is the only part of the code that is actually measured when the benchmark 
  # is run. It should include all and only the code you are interested in benchmarking.
  run = {
    result &lt;- read_csv_arrow(
      input_file, 
      as_data_frame = as_data_frame, 
      skip_empty_rows = skip_empty_rows
    )
  },
  # This is run after each iteration. This is a good time to validate that the
  # benchmark ran correctly.
  after_each = {
    stopifnot(
      "The dimensions do not match" = all.equal(dim(result), result_dim)
    )
    result &lt;- NULL
  },
  # This defines if the parameters are valid. If there are certain combinations
  # that are not valid, add them to drop and they will be excluded from the 
  # benchmark matrix
  valid_params = function(params) {
  
    # Do not allow both skip_empty_rows == FALSE and as_data_frame == TRUE at 
    # the same time
    drop &lt;- ( params$skip_empty_rows == FALSE &amp; params$as_data_frame == TRUE ) 
    
    params[!drop,]
  },
  # This lists any packages that are used by this benchmark so that they can 
  # be installed prior to starting the run. Typically this will be simply "arrow"
  packages_used = function(params) {
    "arrow"
  }
)</code></pre>
<p>And now we could run our benchmark with the following for the default matrix, using 4 cpu cores and 5 iterations per case.</p>
<pre><code><span class="fu"><a href="reference/run_benchmark.html">run_benchmark</a></span><span class="op">(</span>
  <span class="va">new_csv_benchmark</span>,
  cpu_count <span class="op">=</span> <span class="fl">4</span>,
  n_iter <span class="op">=</span> <span class="fl">5</span>
<span class="op">)</span></code></pre>
<p>Or specify parameters (including non-default parameters) with:</p>
<pre><code><span class="fu"><a href="reference/run_benchmark.html">run_benchmark</a></span><span class="op">(</span>
  <span class="va">new_csv_benchmark</span>,
  as_data_frame <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>,
  skip_empty_rows <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>,
  cpu_count <span class="op">=</span> <span class="fl">4</span>,
  n_iter <span class="op">=</span> <span class="fl">5</span>
<span class="op">)</span></code></pre>
</div>
<div class="section level3">
<h3 id="enabling-benchmarks-to-be-run-on-conbench">Enabling benchmarks to be run on conbench<a class="anchor" aria-label="anchor" href="#enabling-benchmarks-to-be-run-on-conbench"></a>
</h3>
<p><a href="https://conbench.ursa.dev/" class="external-link">Conbench</a> is a service that runs benchmarks continuously on a repo. We have a conbench service setup to run benchmarks on the apache/arrow repository (and pull requests, if requested).</p>
<p>Before a benchmark can be run on conbench, one must add a (or extend an existing) benchmark in the <a href="https://github.com/ursacomputing/benchmarks" class="external-link">benchmarks python package</a>. If you are adding a new benchmark <a href="https://github.com/ursacomputing/benchmarks#example-external-benchmarks" class="external-link">see the R-only example external benchmarks</a> in benchmarks. An example of adding an R-only benchmark is <a href="https://github.com/ursacomputing/benchmarks/pull/14" class="external-link">benchmarks#14</a></p>
</div>
</div>
<div class="section level2">
<h2 id="known-data-sources-and-versions">Known data sources and versions<a class="anchor" aria-label="anchor" href="#known-data-sources-and-versions"></a>
</h2>
<p>The package knows about certain large data files to use in benchmarks. These are registered in a <code>known_sources</code> object, which specifies where they can be downloaded and how to read them, as well as optional attributes about them (e.g. <code><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim()</a></code>) that can be used to validate that they’ve been read in correctly.</p>
<p>To use them in benchmarks, use the <code><a href="reference/ensure_source.html">ensure_source()</a></code> function to take a source identifier and mapping it to a file path, downloading and extracting the file if it isn’t found. Pass the result to <code><a href="reference/read_source.html">read_source()</a></code> load the data with the source’s provided <code>reader</code> function.</p>
<p>Source files are cached in a <code>data</code> directory and are only downloaded if not present. This speeds up repeat benchmark runs on the same host. By default, <code>data</code> is assumed to be relative to the current working directory, but you can set the environment variable <code>ARROWBENCH_DATA_DIR</code> to point to another (permanent) base directory.</p>
<p>Similarly, there is an <code>ensure_lib()</code> function called in the <code>global_setup()</code> that supports a list of known <code>arrow</code> package versions, which are mapped to daily snapshots of CRAN hosted by Microsoft. If you specify <code>lib_path = "0.17"</code>, for example, <code>ensure_lib()</code> will use a <code>.libPath</code> for this version and install all Suggested packages into that directory using the MRAN snapshot for “2020-05-29”, a date when 0.17 was the <code>arrow</code> version on CRAN. This lets you test against old versions of the code and to backfill benchmark results.</p>
<p>These versioned R package libraries are cached in an <code>r_libs</code> directory, like <code>data</code> relative to the directory specified by the environment variable <code>ARROWBENCH_LOCAL_DIR</code>.</p>
</div>
<div class="section level2">
<h2 id="results-and-caching">Results and caching<a class="anchor" aria-label="anchor" href="#results-and-caching"></a>
</h2>
<p><code><a href="reference/run_benchmark.html">run_benchmark()</a></code> returns a list of benchmark results, which may be massaged, JSON-serialized, and uploaded to the conbench service. Within an R process, you can call <code><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame()</a></code> on it to get a more manageable view, which can be passed to plotting functions.</p>
<p>In addition to timings, parameter values, and the versions of loaded packages, the benchmark results contain some extra data on memory usage and garbage collection. <code><a href="https://rdrr.io/r/base/gc.html" class="external-link">gc()</a></code> can add significant time to large operations, and while we can’t prevent it, we can at least be aware of when it is happening.</p>
<p>Individual benchmark results (the output of <code><a href="reference/run_one.html">run_one()</a></code>) are cached in a <code>results</code> directory. This way, if the main process running <code><a href="reference/run_benchmark.html">run_benchmark()</a></code> fails or is interrupted in the middle, you can restart. Note however that if you are using the default <code>lib_path</code> <strong>and</strong> are updating the package versions installed there between benchmark runs, you should clear the cache before starting a new run (at least deleting the cached .json files containing “latest” in the file name). The location of this cache is the directory specified by the environment variable <code>ARROWBENCH_LOCAL_DIR</code>. If no environment variable is given, this will default to the current working directory.</p>
</div>
</div>

  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small><a href="https://opensource.org/licenses/mit-license.php" class="external-link">MIT</a> + file <a href="LICENSE-text.html">LICENSE</a></small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing arrowbench</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Neal Richardson <br><small class="roles"> Author </small>  </li>
<li>Jonathan Keane <br><small class="roles"> Author, maintainer </small>  </li>
</ul>
</div>



  </div>
</div>


      <footer><div class="copyright">
  <p></p>
<p>Developed by Neal Richardson, Jonathan Keane.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.3.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
